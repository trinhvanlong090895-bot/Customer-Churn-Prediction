import streamlit as st
import pandas as pd
import numpy as np
import pickle

st.title("üìä AI Customer Churn Prediction")
st.write("D·ª± ƒëo√°n kh√°ch h√†ng c√≥ r·ªùi b·ªè hay kh√¥ng d·ª±a tr√™n m√¥ h√¨nh Machine Learning")

# Load model, scaler v√† feature_names
model = pickle.load(open("model.pkl", "rb"))
scaler = pickle.load(open("scaler.pkl", "rb"))
feature_names = pickle.load(open("feature_names.pkl", "rb"))  # ‚Üê Th√™m d√≤ng n√†y

uploaded_file = st.file_uploader("üì• T·∫£i file CSV Telco Customer Churn", type=["csv"])

if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)

    st.subheader("üìÑ D·ªØ li·ªáu ƒë·∫ßu v√†o:")
    st.dataframe(df.head())

    # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu
    df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors="coerce")
    df = df.dropna()

    df_processed = pd.get_dummies(df, drop_first=True)

    # ƒê·ªìng b·ªô v·ªõi c·ªôt c·ªßa model
    missing_cols = set(feature_names) - set(df_processed.columns)  # ‚Üê S·ª≠a d√≤ng n√†y
    for c in missing_cols:
        df_processed[c] = 0

    df_processed = df_processed[feature_names]  # ‚Üê S·ª≠a d√≤ng n√†y

    # Scale
    X_scaled = scaler.transform(df_processed)

    # Predict
    proba = model.predict_proba(X_scaled)[:, 1]

    df["Churn_Score"] = proba

    st.subheader("üîç K·∫øt qu·∫£ d·ª± ƒëo√°n:")
    st.dataframe(df.sort_values(by="Churn_Score", ascending=False))

    st.subheader("üî• Kh√°ch h√†ng c√≥ nguy c∆° cao (Churn > 0.7):")
    st.dataframe(df[df["Churn_Score"] > 0.7])

    st.bar_chart(df["Churn_Score"])
    import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
import pickle

# --- 1. T·∫£i v√† L√†m s·∫°ch D·ªØ li·ªáu ---
def load_and_clean_data(file_path):
    df = pd.read_csv(file_path)
    
    # X·ª≠ l√Ω c·ªôt TotalCharges: chuy·ªÉn sang s·ªë v√† ƒëi·ªÅn NaN (t·ª´ kh√°ch h√†ng m·ªõi) b·∫±ng 0
    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
    df.dropna(subset=['TotalCharges'], inplace=True)
    
    # Lo·∫°i b·ªè customerID v√† c·ªôt 'gender' (v√¨ √≠t t√°c ƒë·ªông trong m√¥ h√¨nh n√†y)
    df.drop(['customerID', 'gender'], axis=1, inplace=True) 
    
    return df

# --- 2. Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu (Encoding) ---
def preprocess_data(df):
    # Sao ch√©p ƒë·ªÉ tr√°nh c·∫£nh b√°o SettingWithCopyWarning
    df_processed = df.copy()

    # M√£ h√≥a nh·ªã ph√¢n (Yes/No v√† SeniorCitizen)
    binary_cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 
                   'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 
                   'StreamingTV', 'StreamingMovies', 'Churn']
    for col in binary_cols:
        if col in df_processed.columns:
            le = LabelEncoder()
            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p c√≥ 'No phone service' ho·∫∑c 'No internet service'
            unique_vals = df_processed[col].unique()
            if 'No phone service' in unique_vals:
                df_processed[col] = df_processed[col].replace('No phone service', 'No')
            if 'No internet service' in unique_vals:
                df_processed[col] = df_processed[col].replace('No internet service', 'No')
                
            df_processed[col] = le.fit_transform(df_processed[col])

    # M√£ h√≥a One-Hot cho c√°c bi·∫øn ph√¢n lo·∫°i c√≤n l·∫°i
    categorical_cols = ['MultipleLines', 'InternetService', 'Contract', 'PaymentMethod']
    df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)
    
    return df_processed

# --- 3. Hu·∫•n luy·ªán M√¥ h√¨nh ---
def train_model(df_processed):
    # Chia d·ªØ li·ªáu
    X = df_processed.drop('Churn', axis=1)
    y = df_processed['Churn']
    
    # Chu·∫©n h√≥a bi·∫øn s·ªë
    numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
    scaler = StandardScaler()
    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # Hu·∫•n luy·ªán Random Forest Classifier (s·ª≠ d·ª•ng class_weight ƒë·ªÉ x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng l·ªõp)
    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, 
                                   class_weight='balanced')
    model.fit(X_train, y_train)
    
    # ƒê√°nh gi√° m√¥ h√¨nh
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    print(f"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")
    
    return model, X.columns, scaler

# --- Th·ª±c thi v√† L∆∞u tr·ªØ ---
if __name__ == '__main__':
    # ƒê·∫£m b·∫£o file CSV ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n
    file_path = 'WA_Fn-UseC_-Telco-Customer-Churn.csv' 
    
    df_clean = load_and_clean_data(file_path)
    df_preprocessed = preprocess_data(df_clean)
    
    # L∆∞u l·∫°i DataFrame ƒë√£ x·ª≠ l√Ω (c·∫ßn cho Streamlit ƒë·ªÉ d·ª± ƒëo√°n tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu)
    df_preprocessed.to_csv('processed_data.csv', index=False)
    
    model, features, scaler = train_model(df_preprocessed)
    
    # L∆∞u m√¥ h√¨nh, c√°c t√™n c·ªôt v√† scaler
    with open('retention_model.pkl', 'wb') as file:
        pickle.dump({
            'model': model,
            'features': features.tolist(),
            'scaler': scaler
        }, file)
    
    print("Hu·∫•n luy·ªán m√¥ h√¨nh v√† l∆∞u file 'retention_model.pkl' th√†nh c√¥ng.")
