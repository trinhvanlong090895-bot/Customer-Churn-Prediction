import streamlit as st
import pandas as pd
import numpy as np
import pickle

st.title("üìä AI Customer Churn Prediction")
st.write("D·ª± ƒëo√°n kh√°ch h√†ng c√≥ r·ªùi b·ªè hay kh√¥ng d·ª±a tr√™n m√¥ h√¨nh Machine Learning")

# Load model, scaler v√† feature_names
model = pickle.load(open("model.pkl", "rb"))
scaler = pickle.load(open("scaler.pkl", "rb"))
feature_names = pickle.load(open("feature_names.pkl", "rb"))  # ‚Üê Th√™m d√≤ng n√†y

uploaded_file = st.file_uploader("üì• T·∫£i file CSV Telco Customer Churn", type=["csv"])

if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)

    st.subheader("üìÑ D·ªØ li·ªáu ƒë·∫ßu v√†o:")
    st.dataframe(df.head())

    # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu
    df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors="coerce")
    df = df.dropna()

    df_processed = pd.get_dummies(df, drop_first=True)

    # ƒê·ªìng b·ªô v·ªõi c·ªôt c·ªßa model
    missing_cols = set(feature_names) - set(df_processed.columns)  # ‚Üê S·ª≠a d√≤ng n√†y
    for c in missing_cols:
        df_processed[c] = 0

    df_processed = df_processed[feature_names]  # ‚Üê S·ª≠a d√≤ng n√†y

    # Scale
    X_scaled = scaler.transform(df_processed)

    # Predict
    proba = model.predict_proba(X_scaled)[:, 1]

    df["Churn_Score"] = proba

    st.subheader("üîç K·∫øt qu·∫£ d·ª± ƒëo√°n:")
    st.dataframe(df.sort_values(by="Churn_Score", ascending=False))

    st.subheader("üî• Kh√°ch h√†ng c√≥ nguy c∆° cao (Churn > 0.7):")
    st.dataframe(df[df["Churn_Score"] > 0.7])

    st.bar_chart(df["Churn_Score"])
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from io import StringIO

# Thi·∫øt l·∫≠p ch·∫ø ƒë·ªô trang (t√πy ch·ªçn)
st.set_page_config(layout="wide")

# --- M√¥ ph·ªèng D·ªØ li·ªáu v√† Ti·ªÅn x·ª≠ l√Ω  ---
@st.cache_data
def load_and_preprocess_data():
    # Gi·∫£ l·∫≠p d·ªØ li·ªáu Telco Churn CSV 
    data = {
        'customerID':,
        'gender': ['Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male'],
        'SeniorCitizen': ,
        'Partner':,
        'Dependents':,
        'tenure': ,
        'PhoneService':,
        'MultipleLines': ['No phone service', 'No', 'No', 'No phone service', 'No', 'No phone service', 'No'],
        'InternetService':,
        'Contract':,
        'MonthlyCharges': [29.85, 56.95, 53.85, 42.3, 70.7, 52.55, 20.25],
        'TotalCharges': ['29.85', '1889.5', '108.15', '1840.75', '151.65', ' ', ' '], # M√¥ ph·ªèng gi√° tr·ªã tr·ªëng
        'Churn':
    }
    df = pd.DataFrame(data)

    # X·ª≠ l√Ω TotalCharges: Thay th·∫ø kho·∫£ng tr·∫Øng b·∫±ng NaN v√† chuy·ªÉn ƒë·ªïi sang s·ªë
    df = df.replace(' ', np.nan).astype(float)
    # X·ª≠ l√Ω gi√° tr·ªã thi·∫øu (Imputation - v√≠ d·ª•: thay b·∫±ng gi√° tr·ªã trung b√¨nh)
    df.fillna(df.mean(), inplace=True)
    
    # M√£ h√≥a bi·∫øn m·ª•c ti√™u 'Churn'
    df['Churn_Label'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)
    
    # Ch·ªçn c√°c ƒë·∫∑c tr∆∞ng ƒë·ªÉ m√£ h√≥a (bao g·ªìm c·∫£ c√°c bi·∫øn ƒë∆∞·ª£c ph√¢n t√≠ch)
    categorical_features =
    
    # L·∫•y t√™n c·ªôt ch·ªâ s·ªë (Tenure, Charges)
    numerical_features =

    # X√¢y d·ª±ng Pipeline cho ti·ªÅn x·ª≠ l√Ω
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
            ('num', 'passthrough', numerical_features)
        ],
        remainder='drop'
    )
    
    X = df.drop(, axis=1)
    y = df['Churn_Label']
    
    # T√°ch t·∫≠p hu·∫•n luy·ªán (v√¨ ƒë√¢y l√† v√≠ d·ª• minh h·ªça, kh√¥ng c·∫ßn t√°ch test/train nghi√™m ng·∫∑t)
    X_processed = preprocessor.fit_transform(X)
    
    # L·∫•y t√™n c√°c ƒë·∫∑c tr∆∞ng sau khi m√£ h√≥a
    cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)
    final_feature_names = list(cat_feature_names) + numerical_features
    
    return X_processed, y, final_feature_names

X_data, y_labels, feature_names = load_and_preprocess_data()

@st.cache_resource
def train_model(X, y):
    """Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest c∆° b·∫£n."""
    # Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh [13]
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X, y)
    return clf

clf_model = train_model(X_data, y_labels)

def plot_feature_importance(model, feature_names, top_n=10):
    """T√≠nh to√°n v√† tr·ª±c quan h√≥a Gini Importance.[13]"""
    importances = model.feature_importances_
    feature_imp_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values('Importance', ascending=False).head(top_n)

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.barh(feature_imp_df['Feature'], feature_imp_df['Importance'], color='#f63366')
    ax.set_xlabel('ƒêi·ªÉm Quan tr·ªçng Gini (Gini Importance Score)')
    ax.set_title(f'Top {top_n} ƒê·∫∑c tr∆∞ng Quan tr·ªçng D·ª± ƒëo√°n Churn')
    ax.invert_yaxis()
    st.pyplot(fig)

# --- Giao di·ªán Streamlit cho Feature Importance ---
st.header("1. Ph√¢n t√≠ch ƒê·ªông l·ª±c Churn (AI Diagnostics)")
st.subheader("Tr·ª±c quan h√≥a T·∫ßm quan tr·ªçng c·ªßa ƒê·∫∑c tr∆∞ng (Random Forest)")

# Slider ch·ªçn s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng hi·ªÉn th·ªã
top_n_features = st.slider("Ch·ªçn s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng quan tr·ªçng hi·ªÉn th·ªã", 5, len(feature_names), 10)

plot_feature_importance(clf_model, feature_names, top_n_features)
st.markdown("""
S·ª± tr·ª±c quan h√≥a n√†y cho ph√©p c√°c nh√† qu·∫£n l√Ω nhanh ch√≥ng x√°c ƒë·ªãnh c√°c y·∫øu t·ªë th√∫c ƒë·∫©y m√¥ h√¨nh d·ª± ƒëo√°n churn.
C√°c ƒë·∫∑c tr∆∞ng c√≥ ƒëi·ªÉm Gini Importance cao nh·∫•t, nh∆∞ `tenure` v√† c√°c bi·∫øn li√™n quan ƒë·∫øn `Contract`, 
ƒë∆∞·ª£c x√°c nh·∫≠n l√† c√°c ƒë√≤n b·∫©y ch√≠nh trong m√¥ h√¨nh ph√¢n lo·∫°i (nh∆∞ ƒë√£ gi·∫£ ƒë·ªãnh trong ph√¢n t√≠ch d·ªØ li·ªáu m·∫´u ).
""")
